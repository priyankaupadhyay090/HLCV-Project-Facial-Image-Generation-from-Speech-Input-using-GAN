Testing Constant Variables:
PARENT_DIR: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN
TRAIN_MODE: co-train
TRAIN_FLAG: True
MANUAL_SEED: 200
CAPTIONS_PER_IMAGE: 10
TRAIN_BATCH_SIZE: 2
WORKERS: 0
DATA_DIR: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca
IMG_SIZE: 256
DEVICE: cpu
Load filenames from: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/train/sample_filenames.pickle 6
Load filenames from: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/test/sample_filenames.pickle 4
Testing train_loader
N_examples: 6
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/1.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/1/1.npy
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/4.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/4/4.npy
Pad Collate -- item 0 in batch
caption feature.shape: (673, 40)
imgs: tensor([[[ 0.4824,  0.4667,  0.4745,  ...,  0.4667,  0.4667,  0.4667],
         [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4745,  0.4745],
         [ 0.4667,  0.4667,  0.4824,  ...,  0.4980,  0.4902,  0.4745],
         ...,
         [ 0.5294,  0.5608,  0.5529,  ...,  0.7020,  0.7333,  0.7255],
         [ 0.5294,  0.5451,  0.5059,  ...,  0.6941,  0.7098,  0.7098],
         [ 0.5451,  0.5216,  0.4353,  ...,  0.6863,  0.7098,  0.7020]],

        [[ 0.4824,  0.4667,  0.4745,  ...,  0.4510,  0.4510,  0.4588],
         [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4588,  0.4667],
         [ 0.4667,  0.4667,  0.4824,  ...,  0.4824,  0.4745,  0.4667],
         ...,
         [ 0.3020,  0.3255,  0.3020,  ...,  0.3569,  0.3647,  0.3804],
         [ 0.2941,  0.3020,  0.2471,  ...,  0.3569,  0.3569,  0.3647],
         [ 0.3020,  0.2627,  0.1608,  ...,  0.3490,  0.3725,  0.3725]],

        [[ 0.4824,  0.4667,  0.4745,  ...,  0.4588,  0.4588,  0.4667],
         [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4824,  0.4745],
         [ 0.4667,  0.4667,  0.4824,  ...,  0.5059,  0.4980,  0.4980],
         ...,
         [-0.0902, -0.0745, -0.1059,  ...,  0.0039,  0.0353,  0.0431],
         [-0.1059, -0.1059, -0.1686,  ..., -0.0039,  0.0196,  0.0275],
         [-0.1059, -0.1608, -0.2627,  ..., -0.0275,  0.0275,  0.0353]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 1
key: 1
caption input_length: 673
label: 1

Pad Collate -- item 1 in batch
caption feature.shape: (673, 40)
imgs: tensor([[[-0.8510, -0.8510, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
         [-0.8588, -0.8588, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
         [-0.8667, -0.8667, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
         ...,
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

        [[-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
         [-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
         [-0.9059, -0.9059, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
         ...,
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

        [[-0.9373, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
         [-0.9451, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
         [-0.9529, -0.9529, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
         ...,
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 4
key: 4
caption input_length: 273
label: 4

image_input from data loader: tensor([[[[ 0.4824,  0.4667,  0.4745,  ...,  0.4667,  0.4667,  0.4667],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4745,  0.4745],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.4980,  0.4902,  0.4745],
          ...,
          [ 0.5294,  0.5608,  0.5529,  ...,  0.7020,  0.7333,  0.7255],
          [ 0.5294,  0.5451,  0.5059,  ...,  0.6941,  0.7098,  0.7098],
          [ 0.5451,  0.5216,  0.4353,  ...,  0.6863,  0.7098,  0.7020]],

         [[ 0.4824,  0.4667,  0.4745,  ...,  0.4510,  0.4510,  0.4588],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4588,  0.4667],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.4824,  0.4745,  0.4667],
          ...,
          [ 0.3020,  0.3255,  0.3020,  ...,  0.3569,  0.3647,  0.3804],
          [ 0.2941,  0.3020,  0.2471,  ...,  0.3569,  0.3569,  0.3647],
          [ 0.3020,  0.2627,  0.1608,  ...,  0.3490,  0.3725,  0.3725]],

         [[ 0.4824,  0.4667,  0.4745,  ...,  0.4588,  0.4588,  0.4667],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4824,  0.4745],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.5059,  0.4980,  0.4980],
          ...,
          [-0.0902, -0.0745, -0.1059,  ...,  0.0039,  0.0353,  0.0431],
          [-0.1059, -0.1059, -0.1686,  ..., -0.0039,  0.0196,  0.0275],
          [-0.1059, -0.1608, -0.2627,  ..., -0.0275,  0.0275,  0.0353]]],


        [[[-0.8510, -0.8510, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          [-0.8588, -0.8588, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          [-0.8667, -0.8667, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

         [[-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          [-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          [-0.9059, -0.9059, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

         [[-0.9373, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          [-0.9451, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          [-0.9529, -0.9529, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]])
image input shape: torch.Size([2, 3, 256, 256])
0-th item in the train_loader:
audio_input.size: 2
audio_input: tensor([[[-1.9086e+00, -1.2153e+00, -1.0929e+00,  ...,  6.7249e-01,
           1.0458e+00,  8.2747e-01],
         [-1.2811e+00, -9.4098e-01, -6.2601e-01,  ...,  5.9071e-01,
           6.6116e-01,  6.2859e-01],
         [ 2.6327e-01,  4.6808e-01,  8.7056e-01,  ...,  7.0984e-01,
           7.8560e-01,  7.8261e-01],
         ...,
         [-1.4232e+00, -1.7306e+00, -1.7505e+00,  ..., -8.9299e-01,
          -9.2495e-01, -8.8896e-01],
         [-1.3611e+00, -1.7452e+00, -1.9680e+00,  ..., -8.9819e-01,
          -9.2851e-01, -9.1930e-01],
         [-1.2113e+00, -1.4005e+00, -1.6362e+00,  ..., -8.8298e-01,
          -8.9156e-01, -9.0763e-01]],

        [[-1.3967e+00, -1.7408e+00, -1.5673e+00,  ..., -8.5773e-01,
          -8.5048e-01, -8.7756e-01],
         [-8.8256e-01, -3.8418e-01, -1.5987e-02,  ..., -5.7152e-01,
          -4.9168e-01, -6.6965e-01],
         [-3.8281e-02,  5.2989e-01,  1.0469e+00,  ...,  1.5261e-03,
           1.5613e-01,  1.4618e-01],
         ...,
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00]]])
label long tensor?: tensor([1, 4])
input length: tensor([673., 273.])
image_input after squeeze(1): tensor([[[[ 0.4824,  0.4667,  0.4745,  ...,  0.4667,  0.4667,  0.4667],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4745,  0.4745],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.4980,  0.4902,  0.4745],
          ...,
          [ 0.5294,  0.5608,  0.5529,  ...,  0.7020,  0.7333,  0.7255],
          [ 0.5294,  0.5451,  0.5059,  ...,  0.6941,  0.7098,  0.7098],
          [ 0.5451,  0.5216,  0.4353,  ...,  0.6863,  0.7098,  0.7020]],

         [[ 0.4824,  0.4667,  0.4745,  ...,  0.4510,  0.4510,  0.4588],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4588,  0.4667],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.4824,  0.4745,  0.4667],
          ...,
          [ 0.3020,  0.3255,  0.3020,  ...,  0.3569,  0.3647,  0.3804],
          [ 0.2941,  0.3020,  0.2471,  ...,  0.3569,  0.3569,  0.3647],
          [ 0.3020,  0.2627,  0.1608,  ...,  0.3490,  0.3725,  0.3725]],

         [[ 0.4824,  0.4667,  0.4745,  ...,  0.4588,  0.4588,  0.4667],
          [ 0.4745,  0.4667,  0.4667,  ...,  0.4824,  0.4824,  0.4745],
          [ 0.4667,  0.4667,  0.4824,  ...,  0.5059,  0.4980,  0.4980],
          ...,
          [-0.0902, -0.0745, -0.1059,  ...,  0.0039,  0.0353,  0.0431],
          [-0.1059, -0.1059, -0.1686,  ..., -0.0039,  0.0196,  0.0275],
          [-0.1059, -0.1608, -0.2627,  ..., -0.0275,  0.0275,  0.0353]]],


        [[[-0.8510, -0.8510, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          [-0.8588, -0.8588, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          [-0.8667, -0.8667, -0.8353,  ..., -0.8196, -0.8196, -0.8196],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

         [[-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          [-0.8980, -0.8980, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          [-0.9059, -0.9059, -0.9059,  ..., -0.8902, -0.8902, -0.8902],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],

         [[-0.9373, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          [-0.9451, -0.9451, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          [-0.9529, -0.9529, -0.9451,  ..., -0.9608, -0.9608, -0.9608],
          ...,
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]])

Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/2.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/2/2.npy
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/3.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/3/3.npy
Pad Collate -- item 0 in batch
caption feature.shape: (795, 40)
imgs: tensor([[[-1.0000, -1.0000, -1.0000,  ..., -0.4902, -0.5216, -0.4275],
         [-1.0000, -1.0000, -1.0000,  ..., -0.4745, -0.5216, -0.3725],
         [-1.0000, -1.0000, -1.0000,  ..., -0.5451, -0.5059, -0.4118],
         ...,
         [-0.5137, -0.5529, -0.5137,  ..., -0.4902, -0.3098, -0.7333],
         [-0.5608, -0.5216, -0.4118,  ..., -0.7412, -0.3961, -0.2392],
         [-0.5216, -0.4510, -0.3569,  ..., -0.2235, -0.2863, -0.0745]],

        [[-1.0000, -1.0000, -1.0000,  ..., -0.5529, -0.5843, -0.4902],
         [-1.0000, -1.0000, -1.0000,  ..., -0.5216, -0.5686, -0.4118],
         [-1.0000, -1.0000, -1.0000,  ..., -0.5765, -0.5373, -0.4431],
         ...,
         [-0.3098, -0.3412, -0.2784,  ..., -0.5137, -0.3255, -0.7569],
         [-0.3098, -0.2392, -0.0980,  ..., -0.8118, -0.4588, -0.3098],
         [-0.2078, -0.0745,  0.0510,  ..., -0.2941, -0.3647, -0.1529]],

        [[-1.0000, -1.0000, -1.0000,  ..., -0.6392, -0.6627, -0.5765],
         [-1.0000, -1.0000, -1.0000,  ..., -0.6078, -0.6627, -0.5059],
         [-1.0000, -1.0000, -1.0000,  ..., -0.6706, -0.6314, -0.5373],
         ...,
         [-0.6078, -0.6235, -0.5922,  ..., -0.6000, -0.4275, -0.8588],
         [-0.6235, -0.6000, -0.5373,  ..., -0.8824, -0.5373, -0.3804],
         [-0.6235, -0.6000, -0.5608,  ..., -0.3412, -0.4039, -0.1922]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 2
key: 2
caption input_length: 518
label: 2

Pad Collate -- item 1 in batch
caption feature.shape: (795, 40)
imgs: tensor([[[-0.0824, -0.0980, -0.0902,  ..., -0.0431, -0.0510, -0.0431],
         [-0.0745, -0.0824, -0.0980,  ..., -0.0431, -0.0588, -0.0510],
         [-0.0980, -0.0902, -0.1137,  ..., -0.0588, -0.0588, -0.0588],
         ...,
         [-0.7176, -0.6941, -0.6784,  ..., -0.8667, -0.8667, -0.8510],
         [-0.7098, -0.6706, -0.6706,  ..., -0.8745, -0.8667, -0.8588],
         [-0.7020, -0.6549, -0.6706,  ..., -0.8745, -0.8667, -0.8510]],

        [[-0.3882, -0.4039, -0.4039,  ..., -0.3647, -0.3647, -0.3569],
         [-0.3882, -0.3961, -0.4118,  ..., -0.3647, -0.3804, -0.3725],
         [-0.4118, -0.4039, -0.4275,  ..., -0.3804, -0.3804, -0.3804],
         ...,
         [-0.8118, -0.7804, -0.7647,  ..., -0.8980, -0.8980, -0.8824],
         [-0.8039, -0.7569, -0.7569,  ..., -0.9059, -0.8980, -0.8902],
         [-0.7961, -0.7412, -0.7569,  ..., -0.9059, -0.8980, -0.8824]],

        [[-0.1216, -0.1373, -0.1373,  ..., -0.1137, -0.1216, -0.1137],
         [-0.1216, -0.1294, -0.1451,  ..., -0.1137, -0.1294, -0.1216],
         [-0.1451, -0.1373, -0.1608,  ..., -0.1294, -0.1294, -0.1294],
         ...,
         [-0.8118, -0.7961, -0.7961,  ..., -0.9059, -0.9059, -0.8902],
         [-0.8039, -0.7725, -0.7882,  ..., -0.9137, -0.9059, -0.8980],
         [-0.7961, -0.7569, -0.7882,  ..., -0.9137, -0.9059, -0.8902]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 3
key: 3
caption input_length: 795
label: 3

image_input from data loader: tensor([[[[-0.0824, -0.0980, -0.0902,  ..., -0.0431, -0.0510, -0.0431],
          [-0.0745, -0.0824, -0.0980,  ..., -0.0431, -0.0588, -0.0510],
          [-0.0980, -0.0902, -0.1137,  ..., -0.0588, -0.0588, -0.0588],
          ...,
          [-0.7176, -0.6941, -0.6784,  ..., -0.8667, -0.8667, -0.8510],
          [-0.7098, -0.6706, -0.6706,  ..., -0.8745, -0.8667, -0.8588],
          [-0.7020, -0.6549, -0.6706,  ..., -0.8745, -0.8667, -0.8510]],

         [[-0.3882, -0.4039, -0.4039,  ..., -0.3647, -0.3647, -0.3569],
          [-0.3882, -0.3961, -0.4118,  ..., -0.3647, -0.3804, -0.3725],
          [-0.4118, -0.4039, -0.4275,  ..., -0.3804, -0.3804, -0.3804],
          ...,
          [-0.8118, -0.7804, -0.7647,  ..., -0.8980, -0.8980, -0.8824],
          [-0.8039, -0.7569, -0.7569,  ..., -0.9059, -0.8980, -0.8902],
          [-0.7961, -0.7412, -0.7569,  ..., -0.9059, -0.8980, -0.8824]],

         [[-0.1216, -0.1373, -0.1373,  ..., -0.1137, -0.1216, -0.1137],
          [-0.1216, -0.1294, -0.1451,  ..., -0.1137, -0.1294, -0.1216],
          [-0.1451, -0.1373, -0.1608,  ..., -0.1294, -0.1294, -0.1294],
          ...,
          [-0.8118, -0.7961, -0.7961,  ..., -0.9059, -0.9059, -0.8902],
          [-0.8039, -0.7725, -0.7882,  ..., -0.9137, -0.9059, -0.8980],
          [-0.7961, -0.7569, -0.7882,  ..., -0.9137, -0.9059, -0.8902]]],


        [[[-1.0000, -1.0000, -1.0000,  ..., -0.4902, -0.5216, -0.4275],
          [-1.0000, -1.0000, -1.0000,  ..., -0.4745, -0.5216, -0.3725],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5451, -0.5059, -0.4118],
          ...,
          [-0.5137, -0.5529, -0.5137,  ..., -0.4902, -0.3098, -0.7333],
          [-0.5608, -0.5216, -0.4118,  ..., -0.7412, -0.3961, -0.2392],
          [-0.5216, -0.4510, -0.3569,  ..., -0.2235, -0.2863, -0.0745]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.5529, -0.5843, -0.4902],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5216, -0.5686, -0.4118],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5765, -0.5373, -0.4431],
          ...,
          [-0.3098, -0.3412, -0.2784,  ..., -0.5137, -0.3255, -0.7569],
          [-0.3098, -0.2392, -0.0980,  ..., -0.8118, -0.4588, -0.3098],
          [-0.2078, -0.0745,  0.0510,  ..., -0.2941, -0.3647, -0.1529]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.6392, -0.6627, -0.5765],
          [-1.0000, -1.0000, -1.0000,  ..., -0.6078, -0.6627, -0.5059],
          [-1.0000, -1.0000, -1.0000,  ..., -0.6706, -0.6314, -0.5373],
          ...,
          [-0.6078, -0.6235, -0.5922,  ..., -0.6000, -0.4275, -0.8588],
          [-0.6235, -0.6000, -0.5373,  ..., -0.8824, -0.5373, -0.3804],
          [-0.6235, -0.6000, -0.5608,  ..., -0.3412, -0.4039, -0.1922]]]])
image input shape: torch.Size([2, 3, 256, 256])
1-th item in the train_loader:
audio_input.size: 2
audio_input: tensor([[[-0.9714, -1.1095, -1.3005,  ...,  0.2834,  0.0857,  0.0624],
         [-0.3362, -0.4627, -0.6379,  ...,  0.3677,  0.0696,  0.0164],
         [-0.0563,  0.4725,  0.9419,  ...,  0.3451,  0.3237,  0.4808],
         ...,
         [-0.7955, -0.7557, -0.9059,  ..., -0.9146, -0.9244, -0.9091],
         [-1.1497, -0.9221, -1.0178,  ..., -0.9127, -0.9274, -0.9210],
         [-0.8507, -0.9579, -1.2682,  ..., -0.9133, -0.9288, -0.9222]],

        [[-1.8011, -1.0842, -1.1878,  ...,  0.3246,  0.1240,  0.1804],
         [-0.5993, -0.5462, -0.1347,  ...,  0.5930,  0.3369, -0.0243],
         [-0.0268,  0.5162,  1.0427,  ..., -0.0805, -0.0980, -0.2120],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
label long tensor?: tensor([3, 2])
input length: tensor([795., 518.])
image_input after squeeze(1): tensor([[[[-0.0824, -0.0980, -0.0902,  ..., -0.0431, -0.0510, -0.0431],
          [-0.0745, -0.0824, -0.0980,  ..., -0.0431, -0.0588, -0.0510],
          [-0.0980, -0.0902, -0.1137,  ..., -0.0588, -0.0588, -0.0588],
          ...,
          [-0.7176, -0.6941, -0.6784,  ..., -0.8667, -0.8667, -0.8510],
          [-0.7098, -0.6706, -0.6706,  ..., -0.8745, -0.8667, -0.8588],
          [-0.7020, -0.6549, -0.6706,  ..., -0.8745, -0.8667, -0.8510]],

         [[-0.3882, -0.4039, -0.4039,  ..., -0.3647, -0.3647, -0.3569],
          [-0.3882, -0.3961, -0.4118,  ..., -0.3647, -0.3804, -0.3725],
          [-0.4118, -0.4039, -0.4275,  ..., -0.3804, -0.3804, -0.3804],
          ...,
          [-0.8118, -0.7804, -0.7647,  ..., -0.8980, -0.8980, -0.8824],
          [-0.8039, -0.7569, -0.7569,  ..., -0.9059, -0.8980, -0.8902],
          [-0.7961, -0.7412, -0.7569,  ..., -0.9059, -0.8980, -0.8824]],

         [[-0.1216, -0.1373, -0.1373,  ..., -0.1137, -0.1216, -0.1137],
          [-0.1216, -0.1294, -0.1451,  ..., -0.1137, -0.1294, -0.1216],
          [-0.1451, -0.1373, -0.1608,  ..., -0.1294, -0.1294, -0.1294],
          ...,
          [-0.8118, -0.7961, -0.7961,  ..., -0.9059, -0.9059, -0.8902],
          [-0.8039, -0.7725, -0.7882,  ..., -0.9137, -0.9059, -0.8980],
          [-0.7961, -0.7569, -0.7882,  ..., -0.9137, -0.9059, -0.8902]]],


        [[[-1.0000, -1.0000, -1.0000,  ..., -0.4902, -0.5216, -0.4275],
          [-1.0000, -1.0000, -1.0000,  ..., -0.4745, -0.5216, -0.3725],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5451, -0.5059, -0.4118],
          ...,
          [-0.5137, -0.5529, -0.5137,  ..., -0.4902, -0.3098, -0.7333],
          [-0.5608, -0.5216, -0.4118,  ..., -0.7412, -0.3961, -0.2392],
          [-0.5216, -0.4510, -0.3569,  ..., -0.2235, -0.2863, -0.0745]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.5529, -0.5843, -0.4902],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5216, -0.5686, -0.4118],
          [-1.0000, -1.0000, -1.0000,  ..., -0.5765, -0.5373, -0.4431],
          ...,
          [-0.3098, -0.3412, -0.2784,  ..., -0.5137, -0.3255, -0.7569],
          [-0.3098, -0.2392, -0.0980,  ..., -0.8118, -0.4588, -0.3098],
          [-0.2078, -0.0745,  0.0510,  ..., -0.2941, -0.3647, -0.1529]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.6392, -0.6627, -0.5765],
          [-1.0000, -1.0000, -1.0000,  ..., -0.6078, -0.6627, -0.5059],
          [-1.0000, -1.0000, -1.0000,  ..., -0.6706, -0.6314, -0.5373],
          ...,
          [-0.6078, -0.6235, -0.5922,  ..., -0.6000, -0.4275, -0.8588],
          [-0.6235, -0.6000, -0.5373,  ..., -0.8824, -0.5373, -0.3804],
          [-0.6235, -0.6000, -0.5608,  ..., -0.3412, -0.4039, -0.1922]]]])

Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/5.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/5/5.npy
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/0.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/0/0.npy
Pad Collate -- item 0 in batch
caption feature.shape: (792, 40)
imgs: tensor([[[ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
         [ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
         [ 0.2941,  0.3020,  0.2941,  ...,  0.3255,  0.3098,  0.3020],
         ...,
         [ 0.3882,  0.3882,  0.3882,  ..., -0.2471, -0.1922, -0.1529],
         [ 0.3882,  0.3961,  0.3882,  ..., -0.1843, -0.2471, -0.1529],
         [ 0.3804,  0.3804,  0.3804,  ..., -0.0902, -0.2471, -0.1294]],

        [[ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
         [ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
         [ 0.1922,  0.2000,  0.1922,  ...,  0.2235,  0.2078,  0.2000],
         ...,
         [ 0.2941,  0.2863,  0.2784,  ..., -0.5373, -0.4902, -0.4588],
         [ 0.2941,  0.2941,  0.2784,  ..., -0.4745, -0.5451, -0.4745],
         [ 0.2863,  0.2784,  0.2706,  ..., -0.3804, -0.5529, -0.4510]],

        [[ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
         [ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
         [ 0.0667,  0.0745,  0.0667,  ...,  0.0980,  0.0824,  0.0745],
         ...,
         [ 0.1843,  0.1843,  0.1765,  ..., -0.7412, -0.6941, -0.6784],
         [ 0.1843,  0.1843,  0.1765,  ..., -0.6706, -0.7412, -0.6706],
         [ 0.1765,  0.1765,  0.1686,  ..., -0.5765, -0.7255, -0.6392]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 5
key: 5
caption input_length: 438
label: 5

Pad Collate -- item 1 in batch
caption feature.shape: (792, 40)
imgs: tensor([[[-0.2000, -0.0980,  0.2627,  ...,  0.8824,  0.8824,  0.8824],
         [-0.2627, -0.2549, -0.2314,  ...,  0.8824,  0.8824,  0.8824],
         [-0.2863, -0.2549, -0.2863,  ...,  0.8824,  0.8824,  0.8824],
         ...,
         [-0.5216, -0.5529, -0.6000,  ...,  0.2549,  0.3961,  0.4039],
         [-0.5059, -0.5294, -0.5765,  ...,  0.3647,  0.4275,  0.4353],
         [-0.4980, -0.5529, -0.5843,  ...,  0.3490,  0.4431,  0.4510]],

        [[-0.3412, -0.2314,  0.1529,  ...,  0.8824,  0.8824,  0.8824],
         [-0.4275, -0.4275, -0.3961,  ...,  0.8824,  0.8824,  0.8824],
         [-0.4431, -0.4275, -0.4667,  ...,  0.8824,  0.8824,  0.8824],
         ...,
         [-0.6863, -0.6941, -0.7412,  ...,  0.1608,  0.2863,  0.2941],
         [-0.6627, -0.6784, -0.7176,  ...,  0.2392,  0.2941,  0.3098],
         [-0.6627, -0.7098, -0.7255,  ...,  0.2235,  0.3176,  0.3490]],

        [[-0.5216, -0.3725,  0.0431,  ...,  0.8824,  0.8824,  0.8824],
         [-0.6392, -0.6000, -0.5608,  ...,  0.8824,  0.8824,  0.8824],
         [-0.6392, -0.6078, -0.6471,  ...,  0.8824,  0.8824,  0.8824],
         ...,
         [-0.8510, -0.8588, -0.8980,  ..., -0.0118,  0.1529,  0.1843],
         [-0.8353, -0.8353, -0.8745,  ...,  0.1216,  0.1686,  0.2000],
         [-0.8353, -0.8588, -0.8745,  ...,  0.1529,  0.2235,  0.2392]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 0
key: 0
caption input_length: 792
label: 0

image_input from data loader: tensor([[[[-0.2000, -0.0980,  0.2627,  ...,  0.8824,  0.8824,  0.8824],
          [-0.2627, -0.2549, -0.2314,  ...,  0.8824,  0.8824,  0.8824],
          [-0.2863, -0.2549, -0.2863,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.5216, -0.5529, -0.6000,  ...,  0.2549,  0.3961,  0.4039],
          [-0.5059, -0.5294, -0.5765,  ...,  0.3647,  0.4275,  0.4353],
          [-0.4980, -0.5529, -0.5843,  ...,  0.3490,  0.4431,  0.4510]],

         [[-0.3412, -0.2314,  0.1529,  ...,  0.8824,  0.8824,  0.8824],
          [-0.4275, -0.4275, -0.3961,  ...,  0.8824,  0.8824,  0.8824],
          [-0.4431, -0.4275, -0.4667,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.6863, -0.6941, -0.7412,  ...,  0.1608,  0.2863,  0.2941],
          [-0.6627, -0.6784, -0.7176,  ...,  0.2392,  0.2941,  0.3098],
          [-0.6627, -0.7098, -0.7255,  ...,  0.2235,  0.3176,  0.3490]],

         [[-0.5216, -0.3725,  0.0431,  ...,  0.8824,  0.8824,  0.8824],
          [-0.6392, -0.6000, -0.5608,  ...,  0.8824,  0.8824,  0.8824],
          [-0.6392, -0.6078, -0.6471,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.8510, -0.8588, -0.8980,  ..., -0.0118,  0.1529,  0.1843],
          [-0.8353, -0.8353, -0.8745,  ...,  0.1216,  0.1686,  0.2000],
          [-0.8353, -0.8588, -0.8745,  ...,  0.1529,  0.2235,  0.2392]]],


        [[[ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
          [ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
          [ 0.2941,  0.3020,  0.2941,  ...,  0.3255,  0.3098,  0.3020],
          ...,
          [ 0.3882,  0.3882,  0.3882,  ..., -0.2471, -0.1922, -0.1529],
          [ 0.3882,  0.3961,  0.3882,  ..., -0.1843, -0.2471, -0.1529],
          [ 0.3804,  0.3804,  0.3804,  ..., -0.0902, -0.2471, -0.1294]],

         [[ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
          [ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
          [ 0.1922,  0.2000,  0.1922,  ...,  0.2235,  0.2078,  0.2000],
          ...,
          [ 0.2941,  0.2863,  0.2784,  ..., -0.5373, -0.4902, -0.4588],
          [ 0.2941,  0.2941,  0.2784,  ..., -0.4745, -0.5451, -0.4745],
          [ 0.2863,  0.2784,  0.2706,  ..., -0.3804, -0.5529, -0.4510]],

         [[ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
          [ 0.0667,  0.0745,  0.0667,  ...,  0.0980,  0.0824,  0.0745],
          ...,
          [ 0.1843,  0.1843,  0.1765,  ..., -0.7412, -0.6941, -0.6784],
          [ 0.1843,  0.1843,  0.1765,  ..., -0.6706, -0.7412, -0.6706],
          [ 0.1765,  0.1765,  0.1686,  ..., -0.5765, -0.7255, -0.6392]]]])
image input shape: torch.Size([2, 3, 256, 256])
2-th item in the train_loader:
audio_input.size: 2
audio_input: tensor([[[-1.2128, -1.1389, -1.3202,  ..., -0.2767, -0.3528, -0.4979],
         [-0.9165, -1.0135, -1.4074,  ..., -0.1304, -0.3324, -0.4977],
         [-0.0346,  0.5024,  1.0219,  ...,  0.0887, -0.1331,  0.1020],
         ...,
         [-1.0401, -1.3413, -1.3918,  ..., -0.8078, -0.8280, -0.8144],
         [-0.9510, -1.2997, -1.7132,  ..., -0.8135, -0.8302, -0.8297],
         [-0.9703, -1.5227, -1.4162,  ..., -0.8065, -0.8264, -0.8332]],

        [[-0.8152, -0.8741, -1.0517,  ...,  0.6481,  0.8980,  0.9747],
         [-0.3846, -0.4306, -0.1808,  ...,  0.7352,  0.6659,  0.7348],
         [ 0.2668,  0.7084,  1.0518,  ..., -0.1543, -0.0503, -0.1181],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
label long tensor?: tensor([0, 5])
input length: tensor([792., 438.])
image_input after squeeze(1): tensor([[[[-0.2000, -0.0980,  0.2627,  ...,  0.8824,  0.8824,  0.8824],
          [-0.2627, -0.2549, -0.2314,  ...,  0.8824,  0.8824,  0.8824],
          [-0.2863, -0.2549, -0.2863,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.5216, -0.5529, -0.6000,  ...,  0.2549,  0.3961,  0.4039],
          [-0.5059, -0.5294, -0.5765,  ...,  0.3647,  0.4275,  0.4353],
          [-0.4980, -0.5529, -0.5843,  ...,  0.3490,  0.4431,  0.4510]],

         [[-0.3412, -0.2314,  0.1529,  ...,  0.8824,  0.8824,  0.8824],
          [-0.4275, -0.4275, -0.3961,  ...,  0.8824,  0.8824,  0.8824],
          [-0.4431, -0.4275, -0.4667,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.6863, -0.6941, -0.7412,  ...,  0.1608,  0.2863,  0.2941],
          [-0.6627, -0.6784, -0.7176,  ...,  0.2392,  0.2941,  0.3098],
          [-0.6627, -0.7098, -0.7255,  ...,  0.2235,  0.3176,  0.3490]],

         [[-0.5216, -0.3725,  0.0431,  ...,  0.8824,  0.8824,  0.8824],
          [-0.6392, -0.6000, -0.5608,  ...,  0.8824,  0.8824,  0.8824],
          [-0.6392, -0.6078, -0.6471,  ...,  0.8824,  0.8824,  0.8824],
          ...,
          [-0.8510, -0.8588, -0.8980,  ..., -0.0118,  0.1529,  0.1843],
          [-0.8353, -0.8353, -0.8745,  ...,  0.1216,  0.1686,  0.2000],
          [-0.8353, -0.8588, -0.8745,  ...,  0.1529,  0.2235,  0.2392]]],


        [[[ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
          [ 0.2941,  0.2941,  0.2941,  ...,  0.3176,  0.3098,  0.3020],
          [ 0.2941,  0.3020,  0.2941,  ...,  0.3255,  0.3098,  0.3020],
          ...,
          [ 0.3882,  0.3882,  0.3882,  ..., -0.2471, -0.1922, -0.1529],
          [ 0.3882,  0.3961,  0.3882,  ..., -0.1843, -0.2471, -0.1529],
          [ 0.3804,  0.3804,  0.3804,  ..., -0.0902, -0.2471, -0.1294]],

         [[ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
          [ 0.1922,  0.1922,  0.1922,  ...,  0.2157,  0.2078,  0.2000],
          [ 0.1922,  0.2000,  0.1922,  ...,  0.2235,  0.2078,  0.2000],
          ...,
          [ 0.2941,  0.2863,  0.2784,  ..., -0.5373, -0.4902, -0.4588],
          [ 0.2941,  0.2941,  0.2784,  ..., -0.4745, -0.5451, -0.4745],
          [ 0.2863,  0.2784,  0.2706,  ..., -0.3804, -0.5529, -0.4510]],

         [[ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.0902,  0.0824,  0.0745],
          [ 0.0667,  0.0745,  0.0667,  ...,  0.0980,  0.0824,  0.0745],
          ...,
          [ 0.1843,  0.1843,  0.1765,  ..., -0.7412, -0.6941, -0.6784],
          [ 0.1843,  0.1843,  0.1765,  ..., -0.6706, -0.7412, -0.6706],
          [ 0.1765,  0.1765,  0.1686,  ..., -0.5765, -0.7255, -0.6392]]]])

Testing val_loader
N_examples: 4
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/6.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/6/6.npy
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/7.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/7/7.npy
Pad Collate -- item 0 in batch
caption feature.shape: (393, 40)
imgs: tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  0.6627,  0.4588,  0.4353],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.5765,  0.6157],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.3961,  0.5216],
         ...,
         [ 1.0000,  1.0000,  1.0000,  ..., -0.7412, -0.7333, -0.7490],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.7725, -0.7490, -0.7569],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.7647, -0.7176, -0.7490]],

        [[ 1.0000,  1.0000,  1.0000,  ...,  0.8275,  0.6627,  0.6235],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.7804,  0.7804],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.5922,  0.6863],
         ...,
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8431, -0.8588],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]],

        [[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.6314,  0.5922],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.7176,  0.7490,  0.7569],
         [ 1.0000,  1.0000,  1.0000,  ...,  0.7098,  0.5608,  0.6627],
         ...,
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8510, -0.8667],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
         [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 0
key: 6
caption input_length: 393
label: 0

Pad Collate -- item 1 in batch
caption feature.shape: (393, 40)
imgs: tensor([[[ 0.2157,  0.2078,  0.2235,  ...,  0.5059,  0.5059,  0.5059],
         [ 0.2157,  0.2157,  0.2157,  ...,  0.5059,  0.5059,  0.5059],
         [ 0.2157,  0.2157,  0.2157,  ...,  0.5137,  0.5059,  0.5059],
         ...,
         [-0.4196, -0.3725, -0.3412,  ...,  0.5373,  0.5373,  0.5373],
         [-0.4667, -0.3882, -0.3333,  ...,  0.5294,  0.5373,  0.5373],
         [-0.4824, -0.4039, -0.3647,  ...,  0.5294,  0.5373,  0.5373]],

        [[ 0.0667,  0.0588,  0.0745,  ...,  0.3961,  0.3961,  0.3961],
         [ 0.0667,  0.0667,  0.0667,  ...,  0.3961,  0.3961,  0.3961],
         [ 0.0667,  0.0667,  0.0667,  ...,  0.4039,  0.3961,  0.3961],
         ...,
         [-0.5059, -0.4745, -0.4588,  ...,  0.4353,  0.4353,  0.4353],
         [-0.5451, -0.4824, -0.4353,  ...,  0.4275,  0.4353,  0.4353],
         [-0.5608, -0.4902, -0.4667,  ...,  0.4275,  0.4353,  0.4353]],

        [[-0.0667, -0.0745, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
         [-0.0667, -0.0588, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
         [-0.0510, -0.0510, -0.0510,  ...,  0.3176,  0.3098,  0.3098],
         ...,
         [-0.5373, -0.5137, -0.5059,  ...,  0.3647,  0.3647,  0.3647],
         [-0.5608, -0.5137, -0.4824,  ...,  0.3569,  0.3647,  0.3647],
         [-0.5765, -0.5216, -0.5137,  ...,  0.3569,  0.3647,  0.3647]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 1
key: 7
caption input_length: 234
label: 1

image_input from data loader: tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  0.6627,  0.4588,  0.4353],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.5765,  0.6157],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.3961,  0.5216],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7412, -0.7333, -0.7490],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7725, -0.7490, -0.7569],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7647, -0.7176, -0.7490]],

         [[ 1.0000,  1.0000,  1.0000,  ...,  0.8275,  0.6627,  0.6235],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.7804,  0.7804],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.5922,  0.6863],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8431, -0.8588],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]],

         [[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.6314,  0.5922],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7176,  0.7490,  0.7569],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7098,  0.5608,  0.6627],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8510, -0.8667],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]]],


        [[[ 0.2157,  0.2078,  0.2235,  ...,  0.5059,  0.5059,  0.5059],
          [ 0.2157,  0.2157,  0.2157,  ...,  0.5059,  0.5059,  0.5059],
          [ 0.2157,  0.2157,  0.2157,  ...,  0.5137,  0.5059,  0.5059],
          ...,
          [-0.4196, -0.3725, -0.3412,  ...,  0.5373,  0.5373,  0.5373],
          [-0.4667, -0.3882, -0.3333,  ...,  0.5294,  0.5373,  0.5373],
          [-0.4824, -0.4039, -0.3647,  ...,  0.5294,  0.5373,  0.5373]],

         [[ 0.0667,  0.0588,  0.0745,  ...,  0.3961,  0.3961,  0.3961],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.3961,  0.3961,  0.3961],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.4039,  0.3961,  0.3961],
          ...,
          [-0.5059, -0.4745, -0.4588,  ...,  0.4353,  0.4353,  0.4353],
          [-0.5451, -0.4824, -0.4353,  ...,  0.4275,  0.4353,  0.4353],
          [-0.5608, -0.4902, -0.4667,  ...,  0.4275,  0.4353,  0.4353]],

         [[-0.0667, -0.0745, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
          [-0.0667, -0.0588, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
          [-0.0510, -0.0510, -0.0510,  ...,  0.3176,  0.3098,  0.3098],
          ...,
          [-0.5373, -0.5137, -0.5059,  ...,  0.3647,  0.3647,  0.3647],
          [-0.5608, -0.5137, -0.4824,  ...,  0.3569,  0.3647,  0.3647],
          [-0.5765, -0.5216, -0.5137,  ...,  0.3569,  0.3647,  0.3647]]]])
image input shape: torch.Size([2, 3, 256, 256])
0-th item in the val_loader:
audio_input.size: <built-in method size of Tensor object at 0x7fa97dea6dc0>
audio_input: tensor([[[-1.2276, -1.5714, -1.6830,  ..., -0.6563, -0.9319, -0.9312],
         [-1.2624, -1.3276, -1.3653,  ..., -0.1433, -0.4486, -0.4392],
         [-1.2204, -1.2479, -1.2407,  ...,  0.7032,  0.7184,  0.3814],
         ...,
         [-0.7427, -0.9522, -1.1682,  ..., -0.9259, -0.9451, -0.9517],
         [-0.9675, -1.0810, -1.0676,  ..., -0.9399, -0.9567, -0.9533],
         [-0.8941, -0.8407, -0.8878,  ..., -0.9450, -0.9579, -0.9505]],

        [[-2.2316, -2.2797, -1.9936,  ..., -0.5407, -0.7184, -0.8419],
         [-0.8793, -1.0824, -0.7940,  ..., -0.0397,  0.0211,  0.0366],
         [-0.7466,  0.1846,  1.0387,  ...,  0.1639,  0.1760,  0.3816],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
label long tensor?: tensor([0, 1])
input length: tensor([393., 234.])
image_input after squeeze(1): tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  0.6627,  0.4588,  0.4353],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.5765,  0.6157],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.3961,  0.5216],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7412, -0.7333, -0.7490],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7725, -0.7490, -0.7569],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.7647, -0.7176, -0.7490]],

         [[ 1.0000,  1.0000,  1.0000,  ...,  0.8275,  0.6627,  0.6235],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.7804,  0.7804],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7412,  0.5922,  0.6863],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8431, -0.8588],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]],

         [[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.6314,  0.5922],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7176,  0.7490,  0.7569],
          [ 1.0000,  1.0000,  1.0000,  ...,  0.7098,  0.5608,  0.6627],
          ...,
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8588, -0.8510, -0.8667],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8902, -0.8667, -0.8745],
          [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8431, -0.8745]]],


        [[[ 0.2157,  0.2078,  0.2235,  ...,  0.5059,  0.5059,  0.5059],
          [ 0.2157,  0.2157,  0.2157,  ...,  0.5059,  0.5059,  0.5059],
          [ 0.2157,  0.2157,  0.2157,  ...,  0.5137,  0.5059,  0.5059],
          ...,
          [-0.4196, -0.3725, -0.3412,  ...,  0.5373,  0.5373,  0.5373],
          [-0.4667, -0.3882, -0.3333,  ...,  0.5294,  0.5373,  0.5373],
          [-0.4824, -0.4039, -0.3647,  ...,  0.5294,  0.5373,  0.5373]],

         [[ 0.0667,  0.0588,  0.0745,  ...,  0.3961,  0.3961,  0.3961],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.3961,  0.3961,  0.3961],
          [ 0.0667,  0.0667,  0.0667,  ...,  0.4039,  0.3961,  0.3961],
          ...,
          [-0.5059, -0.4745, -0.4588,  ...,  0.4353,  0.4353,  0.4353],
          [-0.5451, -0.4824, -0.4353,  ...,  0.4275,  0.4353,  0.4353],
          [-0.5608, -0.4902, -0.4667,  ...,  0.4275,  0.4353,  0.4353]],

         [[-0.0667, -0.0745, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
          [-0.0667, -0.0588, -0.0588,  ...,  0.3255,  0.3255,  0.3255],
          [-0.0510, -0.0510, -0.0510,  ...,  0.3176,  0.3098,  0.3098],
          ...,
          [-0.5373, -0.5137, -0.5059,  ...,  0.3647,  0.3647,  0.3647],
          [-0.5608, -0.5137, -0.4824,  ...,  0.3569,  0.3647,  0.3647],
          [-0.5765, -0.5216, -0.5137,  ...,  0.3569,  0.3647,  0.3647]]]])

Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/8.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/8/8.npy
Image paht: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/images/9.jpg
caption audio file: /Users/noonscape/Documents/hlcv/project/HLCV-Project-Facial-Image-Generation-from-Speech-Input-using-GAN/preprocess/mmca/audio/mel/9/9.npy
Pad Collate -- item 0 in batch
caption feature.shape: (729, 40)
imgs: tensor([[[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
         ...,
         [-0.6392, -0.6235, -0.5765,  ...,  0.1373,  0.1216,  0.1373],
         [-0.6314, -0.6157, -0.5451,  ...,  0.1451,  0.1294,  0.1216],
         [-0.6157, -0.5686, -0.5137,  ...,  0.1686,  0.1529,  0.1451]],

        [[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
         ...,
         [-0.7647, -0.7647, -0.7569,  ...,  0.0353,  0.0353,  0.0667],
         [-0.7647, -0.7725, -0.7255,  ...,  0.0431,  0.0431,  0.0431],
         [-0.7725, -0.7412, -0.7020,  ...,  0.0667,  0.0588,  0.0588]],

        [[-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9922, -0.9922, -0.9922,  ..., -0.5608, -0.5529, -0.5529],
         [-0.9922, -0.9922, -0.9922,  ..., -0.5529, -0.5451, -0.5451],
         ...,
         [-0.8588, -0.8745, -0.8588,  ..., -0.1059, -0.0980, -0.0667],
         [-0.8667, -0.8745, -0.8275,  ..., -0.0902, -0.0745, -0.0745],
         [-0.8824, -0.8431, -0.7961,  ..., -0.0588, -0.0588, -0.0431]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 2
key: 8
caption input_length: 729
label: 2

Pad Collate -- item 1 in batch
caption feature.shape: (729, 40)
imgs: tensor([[[-0.9922, -0.9922, -0.9843,  ..., -0.2392,  0.0431,  0.2706],
         [-0.9922, -0.9922, -0.9843,  ..., -0.3412, -0.0667,  0.2392],
         [-0.9843, -0.9922, -0.9922,  ..., -0.2941, -0.1451,  0.1529],
         ...,
         [ 0.4039,  0.4039,  0.3804,  ...,  1.0000,  1.0000,  1.0000],
         [ 0.3882,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000],
         [ 0.3961,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000]],

        [[-1.0000, -1.0000, -0.9922,  ..., -0.5216, -0.2863, -0.1059],
         [-1.0000, -1.0000, -0.9922,  ..., -0.6000, -0.3961, -0.1529],
         [-1.0000, -0.9922, -0.9922,  ..., -0.5451, -0.4745, -0.2235],
         ...,
         [-0.7020, -0.7255, -0.7569,  ...,  1.0000,  1.0000,  1.0000],
         [-0.7176, -0.7176, -0.7176,  ...,  1.0000,  1.0000,  1.0000],
         [-0.7098, -0.7176, -0.7098,  ...,  1.0000,  1.0000,  1.0000]],

        [[-1.0000, -1.0000, -1.0000,  ..., -0.6784, -0.5451, -0.4667],
         [-1.0000, -1.0000, -1.0000,  ..., -0.7490, -0.6157, -0.5059],
         [-0.9922, -0.9922, -0.9922,  ..., -0.7647, -0.6627, -0.5529],
         ...,
         [-0.5765, -0.5843, -0.6078,  ...,  1.0000,  1.0000,  1.0000],
         [-0.5922, -0.5765, -0.5765,  ...,  1.0000,  1.0000,  1.0000],
         [-0.5843, -0.5922, -0.5765,  ...,  1.0000,  1.0000,  1.0000]]])
imgs.shape: torch.Size([3, 256, 256])cls_id: 3
key: 9
caption input_length: 165
label: 3

image_input from data loader: tensor([[[[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          ...,
          [-0.6392, -0.6235, -0.5765,  ...,  0.1373,  0.1216,  0.1373],
          [-0.6314, -0.6157, -0.5451,  ...,  0.1451,  0.1294,  0.1216],
          [-0.6157, -0.5686, -0.5137,  ...,  0.1686,  0.1529,  0.1451]],

         [[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          ...,
          [-0.7647, -0.7647, -0.7569,  ...,  0.0353,  0.0353,  0.0667],
          [-0.7647, -0.7725, -0.7255,  ...,  0.0431,  0.0431,  0.0431],
          [-0.7725, -0.7412, -0.7020,  ...,  0.0667,  0.0588,  0.0588]],

         [[-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9922, -0.9922, -0.9922,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9922, -0.9922, -0.9922,  ..., -0.5529, -0.5451, -0.5451],
          ...,
          [-0.8588, -0.8745, -0.8588,  ..., -0.1059, -0.0980, -0.0667],
          [-0.8667, -0.8745, -0.8275,  ..., -0.0902, -0.0745, -0.0745],
          [-0.8824, -0.8431, -0.7961,  ..., -0.0588, -0.0588, -0.0431]]],


        [[[-0.9922, -0.9922, -0.9843,  ..., -0.2392,  0.0431,  0.2706],
          [-0.9922, -0.9922, -0.9843,  ..., -0.3412, -0.0667,  0.2392],
          [-0.9843, -0.9922, -0.9922,  ..., -0.2941, -0.1451,  0.1529],
          ...,
          [ 0.4039,  0.4039,  0.3804,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.3882,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.3961,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000]],

         [[-1.0000, -1.0000, -0.9922,  ..., -0.5216, -0.2863, -0.1059],
          [-1.0000, -1.0000, -0.9922,  ..., -0.6000, -0.3961, -0.1529],
          [-1.0000, -0.9922, -0.9922,  ..., -0.5451, -0.4745, -0.2235],
          ...,
          [-0.7020, -0.7255, -0.7569,  ...,  1.0000,  1.0000,  1.0000],
          [-0.7176, -0.7176, -0.7176,  ...,  1.0000,  1.0000,  1.0000],
          [-0.7098, -0.7176, -0.7098,  ...,  1.0000,  1.0000,  1.0000]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.6784, -0.5451, -0.4667],
          [-1.0000, -1.0000, -1.0000,  ..., -0.7490, -0.6157, -0.5059],
          [-0.9922, -0.9922, -0.9922,  ..., -0.7647, -0.6627, -0.5529],
          ...,
          [-0.5765, -0.5843, -0.6078,  ...,  1.0000,  1.0000,  1.0000],
          [-0.5922, -0.5765, -0.5765,  ...,  1.0000,  1.0000,  1.0000],
          [-0.5843, -0.5922, -0.5765,  ...,  1.0000,  1.0000,  1.0000]]]])
image input shape: torch.Size([2, 3, 256, 256])
1-th item in the val_loader:
audio_input.size: <built-in method size of Tensor object at 0x7fa97deb00a0>
audio_input: tensor([[[-0.8085, -0.7034, -0.4954,  ...,  0.2568,  0.4116,  0.4133],
         [-0.0701,  0.1690,  0.3117,  ...,  0.1678,  0.2061,  0.2700],
         [ 0.1529,  0.8732,  1.3952,  ...,  0.1771,  0.2262,  0.2714],
         ...,
         [-0.9663, -1.0208, -1.0096,  ..., -0.9100, -0.9357, -0.9308],
         [-0.9862, -0.9178, -0.8870,  ..., -0.9091, -0.9351, -0.9272],
         [-1.0830, -1.2776, -1.0263,  ..., -0.9089, -0.9349, -0.9247]],

        [[-2.0969, -2.5077, -2.5311,  ...,  0.7429,  0.8467,  0.9756],
         [-1.4310, -1.1630, -0.6641,  ...,  0.6683,  0.9398,  0.8881],
         [-0.3853,  0.2802,  0.8320,  ...,  0.0697, -0.0482, -0.1588],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
label long tensor?: tensor([2, 3])
input length: tensor([729., 165.])
image_input after squeeze(1): tensor([[[[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          ...,
          [-0.6392, -0.6235, -0.5765,  ...,  0.1373,  0.1216,  0.1373],
          [-0.6314, -0.6157, -0.5451,  ...,  0.1451,  0.1294,  0.1216],
          [-0.6157, -0.5686, -0.5137,  ...,  0.1686,  0.1529,  0.1451]],

         [[-0.9686, -0.9686, -0.9686,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9765, -0.9765, -0.9765,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          ...,
          [-0.7647, -0.7647, -0.7569,  ...,  0.0353,  0.0353,  0.0667],
          [-0.7647, -0.7725, -0.7255,  ...,  0.0431,  0.0431,  0.0431],
          [-0.7725, -0.7412, -0.7020,  ...,  0.0667,  0.0588,  0.0588]],

         [[-0.9843, -0.9843, -0.9843,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9922, -0.9922, -0.9922,  ..., -0.5608, -0.5529, -0.5529],
          [-0.9922, -0.9922, -0.9922,  ..., -0.5529, -0.5451, -0.5451],
          ...,
          [-0.8588, -0.8745, -0.8588,  ..., -0.1059, -0.0980, -0.0667],
          [-0.8667, -0.8745, -0.8275,  ..., -0.0902, -0.0745, -0.0745],
          [-0.8824, -0.8431, -0.7961,  ..., -0.0588, -0.0588, -0.0431]]],


        [[[-0.9922, -0.9922, -0.9843,  ..., -0.2392,  0.0431,  0.2706],
          [-0.9922, -0.9922, -0.9843,  ..., -0.3412, -0.0667,  0.2392],
          [-0.9843, -0.9922, -0.9922,  ..., -0.2941, -0.1451,  0.1529],
          ...,
          [ 0.4039,  0.4039,  0.3804,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.3882,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.3961,  0.3961,  0.4039,  ...,  1.0000,  1.0000,  1.0000]],

         [[-1.0000, -1.0000, -0.9922,  ..., -0.5216, -0.2863, -0.1059],
          [-1.0000, -1.0000, -0.9922,  ..., -0.6000, -0.3961, -0.1529],
          [-1.0000, -0.9922, -0.9922,  ..., -0.5451, -0.4745, -0.2235],
          ...,
          [-0.7020, -0.7255, -0.7569,  ...,  1.0000,  1.0000,  1.0000],
          [-0.7176, -0.7176, -0.7176,  ...,  1.0000,  1.0000,  1.0000],
          [-0.7098, -0.7176, -0.7098,  ...,  1.0000,  1.0000,  1.0000]],

         [[-1.0000, -1.0000, -1.0000,  ..., -0.6784, -0.5451, -0.4667],
          [-1.0000, -1.0000, -1.0000,  ..., -0.7490, -0.6157, -0.5059],
          [-0.9922, -0.9922, -0.9922,  ..., -0.7647, -0.6627, -0.5529],
          ...,
          [-0.5765, -0.5843, -0.6078,  ...,  1.0000,  1.0000,  1.0000],
          [-0.5922, -0.5765, -0.5765,  ...,  1.0000,  1.0000,  1.0000],
          [-0.5843, -0.5922, -0.5765,  ...,  1.0000,  1.0000,  1.0000]]]])

